{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8a880795",
      "metadata": {
        "id": "8a880795"
      },
      "source": [
        "# Mikroskil Data Science Assignment Helper Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b79b397",
      "metadata": {
        "id": "7b79b397"
      },
      "source": [
        "> **Read me first:** This notebook is a teaching aid. It shows how to plan work that satisfies the rubric and scenarios without applying the steps to the confidential dataset. Replace every placeholder with your own implementation, document your choices, and do not submit this file as-is."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bde75a05",
      "metadata": {
        "id": "bde75a05"
      },
      "source": [
        "## How to use this template\n",
        "\n",
        "- Skim the rubric summary to build your personal checklist.\n",
        "- Use the Phase 1 and Phase 2 scaffolding to outline your own notebook.\n",
        "- Each scenario playbook maps the official questions to analysis patterns; adapt them with real data.\n",
        "- Replace every `raise NotImplementedError` block with working code once you understand the intent.\n",
        "- Keep a decision log in the provided cell to document cleaning rules and assumptions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e845e6ff",
      "metadata": {
        "id": "e845e6ff"
      },
      "source": [
        "## Rubric at a glance\n",
        "\n",
        "### Phase 1: Python Structures, Gathering & Cleaning\n",
        "| Criterion | Evidence to produce | Weight |\n",
        "| --- | --- | --- |\n",
        "| Dataset understanding | Load CSV, inspect `head()`, `info()`, and explain meaning of key columns. | 10% |\n",
        "| Raw Python data structures | Lists, tuples, dicts built from a manageable subset (first 5-10 rows). | 20% |\n",
        "| Numpy & pandas structures | Use `np.ndarray`, `pd.Series`, clean base `DataFrame` with correct dtypes. | 15% |\n",
        "| Indexing & subsetting | Demonstrate `.loc`, `.iloc`, boolean filters, aggregations tied to scenario. | 15% |\n",
        "| Data gathering / reshaping | Group, pivot, merge, create derived columns fit for scenario. | 15% |\n",
        "| Data cleaning | Handle types, NaN, outliers, useless columns; justify every choice. | 20% |\n",
        "| Code quality & documentation | Modular, readable code, comments, clear layout. | 5% |\n",
        "\n",
        "### Phase 2: EDA, Visualization, Analysis\n",
        "| Criterion | Evidence to produce | Weight |\n",
        "| --- | --- | --- |\n",
        "| EDA breadth & depth | Scenario-relevant descriptive stats, feature engineering, comparative views. | 20% |\n",
        "| Visualization quality | 3-5 well-chosen plots with titles, labels, readable legends. | 25% |\n",
        "| Scenario-specific metrics | Compute and interpret the required KPIs for your scenario. | 25% |\n",
        "| Interpretation & insights | Write narrative conclusions, highlight limitations. | 20% |\n",
        "| Communication & storytelling | Structured notebook: intro → method → results → recommendations. | 10% |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "946e4acc",
      "metadata": {
        "id": "946e4acc"
      },
      "source": [
        "## Suggested workflow\n",
        "\n",
        "1. Read the scenario brief carefully and highlight required metrics.\n",
        "2. Sketch deliverables for Phase 1 and Phase 2 in your own words.\n",
        "3. Duplicate this helper into a working notebook and remove placeholders.\n",
        "4. Load the dataset, build raw Python structures, then transition to pandas.\n",
        "5. Tackle cleaning tasks before deep analysis; log every decision.\n",
        "6. Build EDA visuals, answer scenario questions, and draft insights.\n",
        "7. Review rubric weights and verify that each item has explicit evidence."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "OLHleKsA2Q0z",
        "outputId": "2a7a9f90-abdc-4f0f-976f-73dbb2c01617"
      },
      "id": "OLHleKsA2Q0z",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2a1593d",
      "metadata": {
        "id": "e2a1593d"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Configure pandas display for wide tables; adjust as needed.\n",
        "pd.options.display.max_columns = 100\n",
        "\n",
        "DATA_PATH = Path(\"drive/MyDrive/data-science/claim_invoice_items_data.csv\")\n",
        "if not DATA_PATH.exists():\n",
        "    raise FileNotFoundError(\"Update DATA_PATH so it points at the invoice-item CSV before running this helper.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a34ba91a",
      "metadata": {
        "id": "a34ba91a"
      },
      "source": [
        "### Data loading helper\n",
        "\n",
        "Replace the placeholder path above and wire up the loader function below. Add `dtype` hints if the CSV has scientific notation or numeric strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "666097d0",
      "metadata": {
        "id": "666097d0"
      },
      "outputs": [],
      "source": [
        "def load_invoice_items(path: Path) -> pd.DataFrame:\n",
        "    \"\"\"Read the raw invoice-item CSV into a pandas DataFrame.\"\"\"\n",
        "    # TODO: adjust dtype arguments or parse_dates once you inspect the CSV header.\n",
        "    df = pd.read_csv(\n",
        "        path,\n",
        "        parse_dates=[\"created_at\", \"updated_at\", \"deleted_at\", \"indate\", \"outdate\"],\n",
        "    )\n",
        "    return df\n",
        "\n",
        "# Uncomment after configuring DATA_PATH\n",
        "raw_df = load_invoice_items(DATA_PATH)\n",
        "raw_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "303925b3",
      "metadata": {
        "id": "303925b3"
      },
      "source": [
        "### Decision log helper\n",
        "\n",
        "Document every material assumption, cleaning rule, or exclusion so the instructor can follow your reasoning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26f6c84e",
      "metadata": {
        "id": "26f6c84e"
      },
      "outputs": [],
      "source": [
        "decision_log = []\n",
        "\n",
        "def log_decision(step: str, choice: str, impact: str) -> None:\n",
        "    \"\"\"Append a structured note to the decision log.\"\"\"\n",
        "    note = {\"step\": step, \"choice\": choice, \"impact\": impact}\n",
        "    decision_log.append(note)\n",
        "\n",
        "# Example placeholder (delete after adding your own entries)\n",
        "log_decision(\"template\", \"Describe your first decision here\", \"Summarize the effect on the analysis\")\n",
        "decision_log"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d893b697",
      "metadata": {
        "id": "d893b697"
      },
      "source": [
        "## Phase 1: Data structures, gathering, and cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e87deee",
      "metadata": {
        "id": "3e87deee"
      },
      "source": [
        "### 1. Dataset understanding (10%)\n",
        "\n",
        "- Load the CSV, inspect `head()`, `info()`, and `describe(include=\"all\")`.\n",
        "- In a markdown cell, explain what each key column (provider, benefit_category, quantity, total_price, icd10_1, etc.) represents.\n",
        "- Clarify the unit of analysis (invoice item) and the meaning of rows vs columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfb4378f",
      "metadata": {
        "id": "dfb4378f"
      },
      "outputs": [],
      "source": [
        "def summarize_dataset(df: pd.DataFrame) -> None:\n",
        "    \"\"\"Template for reporting dataset shape and basic column meanings.\"\"\"\n",
        "    if df is None:\n",
        "        raise ValueError(\"Pass the loaded DataFrame before calling this helper.\")\n",
        "    print(f\"Rows: {df.shape[0]:,}  Columns: {df.shape[1]}\")\n",
        "    display(df.head(3))\n",
        "    display(df.info())\n",
        "    display(df.describe(include=\"all\"))\n",
        "    # TODO: add narrative summary in a markdown cell after running this function.\n",
        "\n",
        "summarize_dataset(raw_df)  # Uncomment once raw_df is defined"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Overview\n",
        "This dataset contains invoice-level healthcare billing data.\n",
        "Each row represents a single invoice item, meaning one medical service, procedure, or medication billed under a particular provider’s invoice.\n",
        "Multiple rows can belong to the same invoice if the patient received several services.\n",
        "\n",
        "## Key Columns\n",
        "- `id` Identifier of each item claims, can appear multiple times if claim has multiple items.\n",
        "- `created_at` Timestamp when the item was captured in the source system.\n",
        "- `claim_id` Claim header identifier grouping related invoice items.\n",
        "- `item_label` Invoice line classification (charge, discount, cancel, placeholder)\n",
        "- `item_type` Source line type.\n",
        "- `matched_name` System data that matched with the line item on the invoice.\n",
        "- `quantity` Quantity billed for the line item.\n",
        "- `tarif_description` Nearest description for hospital charge tariffs item found in tarif reference database\n",
        "- `tarif_price` Tariff amount tied to `tarif_description`.\n",
        "- `total_price` Total billed amount for the item.\n",
        "\n",
        "## Row and Column Representation\n",
        "- Each row represents 1 invoice item (service or medicine product)\n",
        "- Each column represents attribute of that item, such as `item_type`, `quantity`, `tarif_price`, `total_price`, etc."
      ],
      "metadata": {
        "id": "i4ilaqTV3oFP"
      },
      "id": "i4ilaqTV3oFP"
    },
    {
      "cell_type": "markdown",
      "id": "d0a15fe0",
      "metadata": {
        "id": "d0a15fe0"
      },
      "source": [
        "### 2. Raw Python data structures (20%)\n",
        "\n",
        "- Work with a small slice: `sample_records = df.head(10).to_dict(orient=\"records\")`.\n",
        "- Build lists (e.g., provider names), tuples (immutable identifiers), and dicts (e.g., code → description).\n",
        "- Explain in text why each structure suits the scenario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e54c814",
      "metadata": {
        "id": "4e54c814"
      },
      "outputs": [],
      "source": [
        "def build_python_structures(sample_records: list) -> dict:\n",
        "    \"\"\"Return example Python structures built from a small subset of rows.\"\"\"\n",
        "    if not sample_records:\n",
        "        raise ValueError(\"Provide 5-10 records from your DataFrame.\")\n",
        "\n",
        "    providers = [row[\"provider_name\"] for row in sample_records if \"provider_name\" in row]\n",
        "    item_keys = [(row.get(\"id\"), row.get(\"description\")) for row in sample_records]\n",
        "    provider_map = {row.get(\"provider_code\"): row.get(\"provider_name\") for row in sample_records}\n",
        "\n",
        "    structures = {\n",
        "        \"providers_list\": providers,\n",
        "        \"item_keys_tuple\": item_keys[:3],\n",
        "        \"provider_lookup\": provider_map,\n",
        "    }\n",
        "    return structures\n",
        "\n",
        "example_structures = build_python_structures(raw_df.head(10).to_dict(orient=\"records\"))\n",
        "display(example_structures)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why each structure suits the screnario?\n",
        "1. `provider_list` will help to determine which provider contributes the highest total claim cost?\n",
        "2. `item_keys_tuple` will show which items or drug types contribute most to the claim cost\n",
        "3. `provider_lookup` will simplifies grouping and joining for cost summaries"
      ],
      "metadata": {
        "id": "XZ3ClmFm7P4E"
      },
      "id": "XZ3ClmFm7P4E"
    },
    {
      "cell_type": "markdown",
      "id": "bf76b1e1",
      "metadata": {
        "id": "bf76b1e1"
      },
      "source": [
        "### 3. Numpy and pandas structures (15%)\n",
        "\n",
        "- Convert relevant numeric columns to `np.ndarray` and `pd.Series`.\n",
        "- Demonstrate vectorized operations (e.g., `df[\"unit_price\"].to_numpy()` for calculations).\n",
        "- After enforcing dtypes, create a clean working copy `clean_df = df.copy()` for downstream work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c5b80042",
      "metadata": {
        "id": "c5b80042",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "8803ca48-78a9-4df2-cd58-f566a3562dff"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3134591646.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0menforce_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_cols\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_cols\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Create a cleaned copy with enforced numeric and datetime types.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mclean_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumeric_cols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mclean_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"coerce\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "def enforce_dtypes(df: pd.DataFrame, numeric_cols: list, date_cols: list) -> pd.DataFrame:\n",
        "    \"\"\"Create a cleaned copy with enforced numeric and datetime types.\"\"\"\n",
        "    clean_df = df.copy()\n",
        "    for col in numeric_cols:\n",
        "        clean_df[col] = pd.to_numeric(clean_df[col], errors=\"coerce\")\n",
        "    for col in date_cols:\n",
        "        clean_df[col] = pd.to_datetime(clean_df[col], errors=\"coerce\")\n",
        "\n",
        "    # Vectorized numeric calculation\n",
        "    clean_df[\"computed_total\"] = clean_df[\"quantity\"] * clean_df[\"unit_price\"]\n",
        "\n",
        "    # Vectorized date calculation\n",
        "    clean_df[\"los_days\"] = (clean_df[\"outdate\"] - clean_df[\"indate\"]).dt.days\n",
        "\n",
        "    return clean_df\n",
        "\n",
        "numeric_columns = [\"quantity\", \"unit_price\", \"total_price\"]\n",
        "date_columns = [\"created_at\", \"updated_at\", \"deleted_at\", \"indate\", \"outdate\"]\n",
        "clean_df = enforce_dtypes(raw_df, numeric_columns, date_columns)\n",
        "clean_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After listing the numeric colums we can answer the first question inside the given rubric. \" Create a list of all numeric columns and display their min, max, and mean values. \""
      ],
      "metadata": {
        "id": "FUTzt2QCAHzz"
      },
      "id": "FUTzt2QCAHzz"
    },
    {
      "cell_type": "code",
      "source": [
        "def list_numeric_columns_and_stats(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    1) List numeric columns and compute min, max, mean.\n",
        "    Returns a DataFrame indexed by column name with columns: dtype, min, max, mean, non_null_count.\n",
        "    \"\"\"\n",
        "    numeric_cols = numeric_columns\n",
        "    stats = []\n",
        "    for col in numeric_cols:\n",
        "        col_ser = df[col].dropna()\n",
        "        stats.append(\n",
        "            {\n",
        "                \"column\": col,\n",
        "                \"min\": col_ser.min() if not col_ser.empty else np.nan,\n",
        "                \"max\": col_ser.max() if not col_ser.empty else np.nan,\n",
        "                \"mean\": col_ser.mean() if not col_ser.empty else np.nan,\n",
        "            }\n",
        "        )\n",
        "    stats_df = pd.DataFrame(stats).set_index(\"column\")\n",
        "    return stats_df"
      ],
      "metadata": {
        "id": "Ud6jcBTYAIoJ"
      },
      "id": "Ud6jcBTYAIoJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I converted numeric columns like quantity, unit_price, and total_price into both NumPy arrays and pandas Series to demonstrate vectorized computation.\n",
        "NumPy arrays are efficient for numerical operations, while pandas Series retain column labels and indexes, making them easier to align with other data in the DataFrame."
      ],
      "metadata": {
        "id": "VMuRjQAx-6fw"
      },
      "id": "VMuRjQAx-6fw"
    },
    {
      "cell_type": "markdown",
      "id": "d3905bc6",
      "metadata": {
        "id": "d3905bc6"
      },
      "source": [
        "### 4. Indexing and subsetting (15%)\n",
        "\n",
        "- Prepare `.loc` and `.iloc` examples tied to your scenario filters.\n",
        "- Demonstrate boolean filters (e.g., `clean_df[clean_df[\"benefit_category\"] == \"medicine\"]`).\n",
        "- Store at least one filtered subset for later EDA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bf641ce",
      "metadata": {
        "id": "7bf641ce"
      },
      "outputs": [],
      "source": [
        "def demo_indexing(df: pd.DataFrame) -> None:\n",
        "    \"\"\"Showcase loc, iloc, and boolean indexing for rubric evidence.\"\"\"\n",
        "    # TODO: adapt column names and filters to your scenario.\n",
        "    medicine_mask = df[\"benefit_category\"].eq(\"medicine\")\n",
        "    sample_loc = df.loc[medicine_mask, [\"provider_name\", \"quantity\", \"total_price\"]].head()\n",
        "    sample_iloc = df.iloc[:5, :5]\n",
        "    display(sample_loc)\n",
        "    display(sample_iloc)\n",
        "\n",
        "# demo_indexing(clean_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58507afc",
      "metadata": {
        "id": "58507afc"
      },
      "source": [
        "### 5. Data gathering and reshaping (15%)\n",
        "\n",
        "- Use `groupby`, `pivot_table`, or joins to engineer the structures your scenario needs.\n",
        "- Create at least one derived column (e.g., `computed_total = quantity * unit_price`).\n",
        "- If merging external mappings (ICD groupings, region labels), show the helper dict or `Series`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1caa5c66",
      "metadata": {
        "id": "1caa5c66"
      },
      "outputs": [],
      "source": [
        "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Add derived columns expected by multiple scenarios.\"\"\"\n",
        "    engineered = df.copy()\n",
        "    if {\"quantity\", \"unit_price\"}.issubset(engineered.columns):\n",
        "        engineered[\"computed_total\"] = engineered[\"quantity\"] * engineered[\"unit_price\"]\n",
        "    if {\"indate\", \"outdate\"}.issubset(engineered.columns):\n",
        "        engineered[\"los_days\"] = (engineered[\"outdate\"] - engineered[\"indate\"]).dt.days\n",
        "    return engineered\n",
        "\n",
        "# engineered_df = engineer_features(clean_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c09674e",
      "metadata": {
        "id": "1c09674e"
      },
      "source": [
        "### 6. Data cleaning (20%)\n",
        "\n",
        "- Detect missing values with `df.isna().sum()` and describe treatment strategies.\n",
        "- Address negatives or impossible values (drop, cap, absolute) and justify in the decision log.\n",
        "- Drop low-value columns (`deleted_at`, `estimated_pack_prices`) with a note explaining why."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7270883c",
      "metadata": {
        "id": "7270883c"
      },
      "outputs": [],
      "source": [
        "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Template for handling missingness, negatives, and redundant columns.\"\"\"\n",
        "    cleaned = df.copy()\n",
        "    summary_missing = cleaned.isna().sum()\n",
        "    print(\"Missing values summary (top 10):\")\n",
        "    display(summary_missing.head(10))\n",
        "\n",
        "    # TODO: implement scenario-appropriate rules below.\n",
        "    # Example placeholders: remove columns, flag negatives, fill nulls.\n",
        "    # cleaned = cleaned.drop(columns=[\"deleted_at\", \"estimated_pack_prices\"], errors=\"ignore\")\n",
        "    # cleaned.loc[cleaned[\"quantity\"] < 0, \"quantity_flag\"] = \"negative\"\n",
        "\n",
        "    raise NotImplementedError(\"Replace placeholder rules with your cleaning logic and document decisions.\")\n",
        "\n",
        "# final_df = clean_data(engineered_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d178b62",
      "metadata": {
        "id": "8d178b62"
      },
      "source": [
        "### 7. Code quality and documentation (5%)\n",
        "\n",
        "- Refactor repetitive tasks into functions (as scaffolded above).\n",
        "- Add short comments before non-obvious logic.\n",
        "- Use markdown headings to organize narrative: context, methods, results, insights.\n",
        "- Double-check notebook execution order before submission."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea04ac4d",
      "metadata": {
        "id": "ea04ac4d"
      },
      "source": [
        "## Phase 2: EDA, visualization, and interpretation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1de0b3d1",
      "metadata": {
        "id": "1de0b3d1"
      },
      "source": [
        "### EDA toolkit\n",
        "\n",
        "Use the helpers below as starting points. Swap in scenario-specific filters or columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8a0a74e",
      "metadata": {
        "id": "a8a0a74e"
      },
      "outputs": [],
      "source": [
        "def describe_numeric(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n",
        "    \"\"\"Return descriptive statistics for selected numeric columns.\"\"\"\n",
        "    stats = df[columns].describe().T\n",
        "    stats[\"missing\"] = df[columns].isna().sum()\n",
        "    return stats\n",
        "\n",
        "\n",
        "def plot_distribution(df: pd.DataFrame, column: str, kind: str = \"hist\") -> None:\n",
        "    \"\"\"Plot a basic distribution for the chosen column.\"\"\"\n",
        "    if kind == \"hist\":\n",
        "        sns.histplot(df[column].dropna(), kde=False)\n",
        "    elif kind == \"box\":\n",
        "        sns.boxplot(x=df[column].dropna())\n",
        "    else:\n",
        "        raise ValueError(\"kind must be 'hist' or 'box'\")\n",
        "    plt.title(f\"Distribution of {column}\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_grouped_bar(df: pd.DataFrame, index_col: str, value_col: str, top_n: int = 10) -> None:\n",
        "    \"\"\"Plot top N categories by aggregated value.\"\"\"\n",
        "    summary = (\n",
        "        df.groupby(index_col)[value_col]\n",
        "        .sum()\n",
        "        .sort_values(ascending=False)\n",
        "        .head(top_n)\n",
        "    )\n",
        "    summary.plot(kind=\"bar\")\n",
        "    plt.ylabel(value_col)\n",
        "    plt.title(f\"Top {top_n} {index_col} by {value_col}\")\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6422f0c",
      "metadata": {
        "id": "c6422f0c"
      },
      "source": [
        "### Insights narrative\n",
        "\n",
        "- After generating visuals, write 2-3 paragraphs connecting findings to the business question.\n",
        "- Call out limitations, data quirks, or assumptions (e.g., synthetic sample, single-claim scope).\n",
        "- End with actionable recommendations or next steps."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcb51eb9",
      "metadata": {
        "id": "fcb51eb9"
      },
      "source": [
        "## Scenario playbooks\n",
        "\n",
        "Each playbook summarizes the official brief. Follow the steps using your cleaned DataFrame. The goal is to show your reasoning path, not to memorize outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eac9d143",
      "metadata": {
        "id": "eac9d143"
      },
      "source": [
        "### Scenario 1 – Pharmacy Cost Optimization\n",
        "\n",
        "**Role reminder:** Pharmacy Cost Analyst focusing on medicine items.\n",
        "\n",
        "**Rubric emphasis:** Negative quantity/price handling, reconstructed totals, scenario-tailored metrics.\n",
        "\n",
        "**Recommended flow:**\n",
        "1. Filter to `benefit_category == \"medicine\"` and `sub_benefit_name == \"Obat-Obatan\"`.\n",
        "2. Build raw Python structures from the filtered subset (list of `chemical_substance`, tuple `(kfa_code, generic_name)`, dict `item_label -> chemical_substance`).\n",
        "3. Convert cost fields to numeric, flag negatives, create `computed_total`, and log decisions.\n",
        "4. Aggregate cost by item, provider, and province; compute contribution percentages.\n",
        "5. Plot distributions (histogram/boxplot) and top cost drivers.\n",
        "6. Summarize anomalies and draft 3-5 recommendations.\n",
        "\n",
        "**Answering the official questions:**\n",
        "1. Use raw Python list comprehension over the filtered subset to capture unique `chemical_substance`.\n",
        "2. Create tuples for immutable identifiers and explain immutability in a markdown cell.\n",
        "3. Build dict via `{row['item_label']: row['chemical_substance'] for row in subset}` and count entries.\n",
        "4. After loading data, print `df.shape` and interpret rows vs columns.\n",
        "5. Cast numeric columns and note parsing issues (e.g., commas, strings).\n",
        "6. Locate negative values with boolean filters and explain treatment.\n",
        "7. Compare `computed_total` and `total_price` via a new column or `.assign()`.\n",
        "8. Justify column drops (e.g., `estimated_pack_prices`).\n",
        "9. Demonstrate `.loc` for provider/province slices.\n",
        "10. Conclude with a markdown summary of data quality findings."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b020ddf3",
      "metadata": {
        "id": "b020ddf3"
      },
      "source": [
        "### Scenario 2 – Length of Stay & Inpatient Tariff Consistency\n",
        "\n",
        "**Role reminder:** Hospital Network Performance Analyst.\n",
        "\n",
        "**Rubric emphasis:** Accurate LOS calculation, tariff handling, date cleaning.\n",
        "\n",
        "**Recommended flow:**\n",
        "1. Parse `indate` and `outdate` as datetime and compute `los_days`.\n",
        "2. Store first 10 LOS values in a list; create tuples `(card_number, indate, outdate)`.\n",
        "3. Build provider metadata dict with nested attributes.\n",
        "4. Handle invalid or negative LOS, missing tarif values, and document rationale.\n",
        "5. Create summary tables for tariff by provider and LOS buckets.\n",
        "6. Visualize LOS distributions and tariff vs LOS relationships.\n",
        "\n",
        "**Answering the official questions:**\n",
        "- Use `.head()` plus `.apply()` to compute the LOS list.\n",
        "- Explain tuple immutability for patient stay IDs in markdown.\n",
        "- Construct provider dict via `.drop_duplicates()` then `.set_index().to_dict(orient=\"index\")`.\n",
        "- Count invalid dates with `df['indate'].isna().sum()` etc.\n",
        "- Filter rows with negative LOS and describe fixes.\n",
        "- Convert `tarif_price` to numeric, treat `tarif_description == '-'` as missing.\n",
        "- Produce average tariff per provider with `groupby`.\n",
        "- Showcase indexing for `los_days > 5`.\n",
        "- Document every cleaning step in a dedicated markdown section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b47367d",
      "metadata": {
        "id": "9b47367d"
      },
      "source": [
        "### Scenario 3 – Benefit Category Utilization Across Countries\n",
        "\n",
        "**Role reminder:** Benefit Design Analyst comparing benefit mix across provinces.\n",
        "\n",
        "**Rubric emphasis:** Accurate aggregation by benefit dimensions and comparative visuals.\n",
        "\n",
        "**Recommended flow:**\n",
        "1. List unique `benefit_category` values and create tuples `(benefit_category, sub_benefit_name)`.\n",
        "2. Map province to benefit categories using dicts or `defaultdict(list)`.\n",
        "3. Clean cost columns, drop irrelevant fields, and create aggregated tables.\n",
        "4. Produce stacked/clustered bars, pies, and heatmaps showing cost distribution.\n",
        "5. Interpret which provinces or benefit categories drive costs and why.\n",
        "\n",
        "**Answering the official questions:**\n",
        "- Use Python list comprehension for unique benefit categories.\n",
        "- Demonstrate tuple creation and show sample output in markdown.\n",
        "- Build province → benefits dict via groupby and `.unique()`.\n",
        "- Report data type checks and missingness for benefit columns.\n",
        "- Aggregate cost by benefit and province for metrics and visuals.\n",
        "- Provide narrative on cost shares, outliers, correlations, and recommendations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe0be936",
      "metadata": {
        "id": "fe0be936"
      },
      "source": [
        "### Scenario 4 – ICD-10 Case Mix Profiling\n",
        "\n",
        "**Role reminder:** Clinical Data Analyst profiling diagnoses and costs.\n",
        "\n",
        "**Rubric emphasis:** ICD grouping dict, missingness analysis, clinical insights.\n",
        "\n",
        "**Recommended flow:**\n",
        "1. Inspect ICD columns for missingness and convert to string.\n",
        "2. Create list of frequent `icd10_1` values and tuples `(icd10_1, description_1)`.\n",
        "3. Build a manual ICD grouping dict and apply it safely.\n",
        "4. Engineer `diagnosis_count` from non-null ICD columns.\n",
        "5. Aggregate counts and costs by ICD code and group; visualize distributions.\n",
        "6. Compare costs by diagnosis count and discuss clinical insights with limitations.\n",
        "\n",
        "**Answering the official questions:**\n",
        "- Use `.value_counts().head(10)` for top ICD codes.\n",
        "- Pair codes and descriptions in tuples and explain usage.\n",
        "- Count missing values and describe handling strategy.\n",
        "- Plot top ICDs and cost distributions using bar/box charts.\n",
        "- Discuss anomalies, multi-diagnosis patterns, and future data needs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "693135c0",
      "metadata": {
        "id": "693135c0"
      },
      "source": [
        "### Scenario 5 – Member-Level Exposure & Cost per Member\n",
        "\n",
        "**Role reminder:** Member Analytics Data Scientist aggregating items to members.\n",
        "\n",
        "**Rubric emphasis:** Correct member-level aggregation, segmentation by demographics.\n",
        "\n",
        "**Recommended flow:**\n",
        "1. Extract unique member identifiers and confirm no duplicates.\n",
        "2. Build tuples `(card_number, member_name)` and dict `card_number -> sex`.\n",
        "3. Aggregate to member level: total cost, item count, distinct providers, average unit price.\n",
        "4. Investigate missing or inconsistent member data, document assumptions (e.g., `dob` as year).\n",
        "5. Visualize distributions and compare metrics by sex; identify outliers.\n",
        "6. Produce recommendations for underwriting or care management.\n",
        "\n",
        "**Answering the official questions:**\n",
        "- Use `.unique()` for card numbers and `.duplicated()` to check inconsistencies.\n",
        "- Build per-member summary with `groupby('card_number').agg(...)`.\n",
        "- Filter and visualize outliers, scatter item count vs cost, boxplots by sex.\n",
        "- Discuss utilization patterns and actionable insights."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b927cc0",
      "metadata": {
        "id": "7b927cc0"
      },
      "source": [
        "### Scenario 6 – Fraud, Waste, & Abuse Screening\n",
        "\n",
        "**Role reminder:** Special Investigations Unit analyst flagging anomalies.\n",
        "\n",
        "**Rubric emphasis:** Strong cleaning, anomaly rules, interpretable watch list.\n",
        "\n",
        "**Recommended flow:**\n",
        "1. Identify rows with `quantity <= 0` or `total_price <= 0`; store in a list.\n",
        "2. Create tuples `(provider_code, benefit_category, description)` for suspicious items.\n",
        "3. Build dict `provider_code -> suspicious_count`.\n",
        "4. Define outlier rule for `unit_price` (IQR or percentile) and create `is_suspicious`.\n",
        "5. Visualize distributions and provider counts; compute cost share of suspicious rows.\n",
        "6. Summarize watch list and limitations (signals, not proof).\n",
        "\n",
        "**Answering the official questions:**\n",
        "- Use boolean indexing to list suspicious rows.\n",
        "- Count anomalies per provider with `groupby`.\n",
        "- Plot boxplots and scatterplots highlighting suspicious points.\n",
        "- Quantify cost share and document reasoning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f64f6ff",
      "metadata": {
        "id": "1f64f6ff"
      },
      "source": [
        "### Scenario 7 – Provider Segmentation & Service Mix\n",
        "\n",
        "**Role reminder:** Provider Network Analyst segmenting providers.\n",
        "\n",
        "**Rubric emphasis:** Provider-level aggregation, segmentation logic, clear visuals.\n",
        "\n",
        "**Recommended flow:**\n",
        "1. List unique providers and confirm metadata consistency.\n",
        "2. Create tuples `(provider_code, provider_name, province)` and dictionaries summarizing cost by category.\n",
        "3. Aggregate metrics: total cost, item counts, member counts, cost share by benefit.\n",
        "4. Define segmentation rules (e.g., >60% medicine cost → medication-heavy).\n",
        "5. Visualize stacked bars and scatterplots by segment.\n",
        "6. Interpret segment traits and propose actions.\n",
        "\n",
        "**Answering the official questions:**\n",
        "- Compute cost shares, classify providers, and count membership per segment.\n",
        "- Plot stacked bars and scatter of total cost vs members colored by segment.\n",
        "- Highlight top cost providers and discuss regional differences."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea60bcb5",
      "metadata": {
        "id": "ea60bcb5"
      },
      "source": [
        "### Scenario 8 – Claim Cost Prediction & Feature Insight\n",
        "\n",
        "**Role reminder:** Data analyst supporting predictive modeling.\n",
        "\n",
        "**Rubric emphasis:** Comprehensive cleaning, feature engineering, insight into predictors.\n",
        "\n",
        "**Recommended flow:**\n",
        "1. Inventory numeric columns, data types, and missingness.\n",
        "2. Drop redundant columns, enforce numeric and datetime types.\n",
        "3. Engineer features: `claim_total_cost`, `benefit_diversity`, `los_days`, item counts.\n",
        "4. Validate aggregations (no data loss) and build correlation visuals.\n",
        "5. Identify key drivers, outliers, and recommend modeling features.\n",
        "6. Write a readiness narrative summarizing data quality and insights.\n",
        "\n",
        "**Answering the official questions:**\n",
        "- Use descriptive stats, missingness dictionary, and column profiling.\n",
        "- Create aggregation pipelines for claim-level metrics.\n",
        "- Plot distributions, heatmaps, scatterplots, and interpret correlations.\n",
        "- Prioritize predictive features and articulate business implications."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12b69f6a",
      "metadata": {
        "id": "12b69f6a"
      },
      "source": [
        "## Final checklist before submission\n",
        "\n",
        "- [ ] Every Phase 1 rubric item has visible evidence (code + explanation).\n",
        "- [ ] Every Phase 2 rubric item has relevant plots and written interpretation.\n",
        "- [ ] Scenario-specific questions are answered in code and markdown, referencing figures.\n",
        "- [ ] Decision log is complete and reflects cleaning choices.\n",
        "- [ ] Notebook runs top-to-bottom without placeholder errors.\n",
        "- [ ] Conclusions include limitations and next-step ideas."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}